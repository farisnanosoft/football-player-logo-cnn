{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farisnanosoft/football-player-logo-cnn/blob/main/players_fifa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "MHWPbB7QM4Lb"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VB0t_PnQQoKk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e4c1396f-328f-433b-fb68-dc3a4fc161c4"
      },
      "cell_type": "code",
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/drive', force_remount=True)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Sjg2wpPVD8w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "803db2ea-6213-4b23-8110-7beba6b960f8"
      },
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GEX3exwzTndU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "432c363c-3d5b-486e-add7-b1c4131da9c2"
      },
      "cell_type": "code",
      "source": [
        "!pip uninstall keras\n",
        "!pip install -I keras==2.1.6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Keras-2.2.4:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/Keras-2.2.4.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.6/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.2.4\n",
            "Collecting keras==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
            "\u001b[K    100% |████████████████████████████████| 348kB 24.9MB/s \n",
            "\u001b[?25hCollecting six>=1.9.0 (from keras==2.1.6)\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting scipy>=0.14 (from keras==2.1.6)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/5f/c48860704092933bf1c4c1574a8de1ffd16bf4fde8bab190d747598844b2/scipy-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (24.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 24.8MB 2.0MB/s \n",
            "\u001b[?25hCollecting h5py (from keras==2.1.6)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.8MB 6.7MB/s \n",
            "\u001b[?25hCollecting pyyaml (from keras==2.1.6)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/a3/1d13970c3f36777c583f136c136f804d70f500168edc1edea6daa7200769/PyYAML-3.13.tar.gz (270kB)\n",
            "\u001b[K    100% |████████████████████████████████| 276kB 29.7MB/s \n",
            "\u001b[?25hCollecting numpy>=1.9.1 (from keras==2.1.6)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.3MB 2.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ad/da/0c/74eb680767247273e2cf2723482cb9c924fe70af57c334513f\n",
            "Successfully built pyyaml\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mjupyter-console 6.0.0 has requirement prompt-toolkit<2.1.0,>=2.0.0, but you'll have prompt-toolkit 1.0.15 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 0.0.1a1 has requirement six~=1.11.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, numpy, scipy, h5py, pyyaml, keras\n",
            "Successfully installed h5py-2.9.0 keras-2.1.6 numpy-1.16.1 pyyaml-3.13 scipy-1.2.1 six-1.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "numpy",
                  "scipy",
                  "six",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "n4ihYnueM5p1"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cnz_7R6oM-k2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "39307548-d789-4cb1-9fee-42e8b58b6d38"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/fifa_players/\")\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deploy.prototxt.txt   players\n",
            "detected_video.mp4    players_fifa.ipynb\n",
            "face_recog_verify.h5  __pycache__\n",
            "font\t\t      res10_300x300_ssd_iter_140000.caffemodel\n",
            "keras\t\t      top_10.mp4\n",
            "keras_retinanet       yad2k\n",
            "logo_model.h5\t      yolo_utils.py\n",
            "model_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h7tMQVeFNAcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d401fede-8eb9-4911-924b-53c25c6f4ee6"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1jUwIN4nXA-U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "6054322b-ebd3-4b87-aa74-bc1594789057"
      },
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "  Using cached https://files.pythonhosted.org/packages/f5/bf/4981bcbee43934f0adb8f764a1e70ab0ee5a448f6505bd04a87a2fda2a8b/numpy-1.16.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "Successfully installed numpy-1.16.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xzNxs5TUQZ3V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d7ea45e-208c-4146-f299-f6f773d2ca2b"
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/fifa_players\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T5dEQ67xGCJW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "128b73ce-6c05-402c-f145-3827d28fe113"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)\n",
        "K.clear_session()\n",
        "#from fr_utils import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hc49Gf1008v2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "d7e3ca5f-d0d3-447d-8d16-40fb8b957ab3"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/keras-team/keras.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 31053 (delta 6), reused 0 (delta 0), pack-reused 31042\u001b[K\n",
            "Receiving objects: 100% (31053/31053), 12.14 MiB | 4.05 MiB/s, done.\n",
            "Resolving deltas: 100% (22615/22615), done.\n",
            "Checking out files: 100% (260/260), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5fb3xYpB5Kt_"
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"../\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHtyRgMB5o52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "38e140ab-1058-4730-c3d8-4fc66e9101ca"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deploy.prototxt.txt   players\n",
            "detected_video.mp4    players_fifa.ipynb\n",
            "face_recog_verify.h5  __pycache__\n",
            "font\t\t      res10_300x300_ssd_iter_140000.caffemodel\n",
            "keras\t\t      top_10.mp4\n",
            "keras_retinanet       yad2k\n",
            "logo_model.h5\t      yolo_utils.py\n",
            "model_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-CcXiR3O5SL-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6344
        },
        "outputId": "8cbb2075-8059-47c7-b7f3-9620780f22e5"
      },
      "cell_type": "code",
      "source": [
        "!sudo python setup.py install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating Keras.egg-info\n",
            "writing Keras.egg-info/PKG-INFO\n",
            "writing dependency_links to Keras.egg-info/dependency_links.txt\n",
            "writing requirements to Keras.egg-info/requires.txt\n",
            "writing top-level names to Keras.egg-info/top_level.txt\n",
            "writing manifest file 'Keras.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'Keras.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/docs\n",
            "copying docs/structure.py -> build/lib/docs\n",
            "copying docs/autogen.py -> build/lib/docs\n",
            "copying docs/__init__.py -> build/lib/docs\n",
            "creating build/lib/keras\n",
            "copying keras/constraints.py -> build/lib/keras\n",
            "copying keras/objectives.py -> build/lib/keras\n",
            "copying keras/activations.py -> build/lib/keras\n",
            "copying keras/initializers.py -> build/lib/keras\n",
            "copying keras/regularizers.py -> build/lib/keras\n",
            "copying keras/__init__.py -> build/lib/keras\n",
            "copying keras/losses.py -> build/lib/keras\n",
            "copying keras/models.py -> build/lib/keras\n",
            "copying keras/optimizers.py -> build/lib/keras\n",
            "copying keras/callbacks.py -> build/lib/keras\n",
            "copying keras/metrics.py -> build/lib/keras\n",
            "creating build/lib/keras/legacy\n",
            "copying keras/legacy/interfaces.py -> build/lib/keras/legacy\n",
            "copying keras/legacy/layers.py -> build/lib/keras/legacy\n",
            "copying keras/legacy/__init__.py -> build/lib/keras/legacy\n",
            "creating build/lib/keras/preprocessing\n",
            "copying keras/preprocessing/image.py -> build/lib/keras/preprocessing\n",
            "copying keras/preprocessing/text.py -> build/lib/keras/preprocessing\n",
            "copying keras/preprocessing/__init__.py -> build/lib/keras/preprocessing\n",
            "copying keras/preprocessing/sequence.py -> build/lib/keras/preprocessing\n",
            "creating build/lib/keras/wrappers\n",
            "copying keras/wrappers/scikit_learn.py -> build/lib/keras/wrappers\n",
            "copying keras/wrappers/__init__.py -> build/lib/keras/wrappers\n",
            "creating build/lib/keras/applications\n",
            "copying keras/applications/imagenet_utils.py -> build/lib/keras/applications\n",
            "copying keras/applications/mobilenet.py -> build/lib/keras/applications\n",
            "copying keras/applications/inception_resnet_v2.py -> build/lib/keras/applications\n",
            "copying keras/applications/__init__.py -> build/lib/keras/applications\n",
            "copying keras/applications/resnet.py -> build/lib/keras/applications\n",
            "copying keras/applications/vgg16.py -> build/lib/keras/applications\n",
            "copying keras/applications/mobilenet_v2.py -> build/lib/keras/applications\n",
            "copying keras/applications/resnext.py -> build/lib/keras/applications\n",
            "copying keras/applications/resnet50.py -> build/lib/keras/applications\n",
            "copying keras/applications/inception_v3.py -> build/lib/keras/applications\n",
            "copying keras/applications/vgg19.py -> build/lib/keras/applications\n",
            "copying keras/applications/xception.py -> build/lib/keras/applications\n",
            "copying keras/applications/nasnet.py -> build/lib/keras/applications\n",
            "copying keras/applications/densenet.py -> build/lib/keras/applications\n",
            "copying keras/applications/resnet_v2.py -> build/lib/keras/applications\n",
            "copying keras/applications/mobilenetv2.py -> build/lib/keras/applications\n",
            "creating build/lib/keras/layers\n",
            "copying keras/layers/recurrent.py -> build/lib/keras/layers\n",
            "copying keras/layers/pooling.py -> build/lib/keras/layers\n",
            "copying keras/layers/cudnn_recurrent.py -> build/lib/keras/layers\n",
            "copying keras/layers/local.py -> build/lib/keras/layers\n",
            "copying keras/layers/embeddings.py -> build/lib/keras/layers\n",
            "copying keras/layers/wrappers.py -> build/lib/keras/layers\n",
            "copying keras/layers/normalization.py -> build/lib/keras/layers\n",
            "copying keras/layers/__init__.py -> build/lib/keras/layers\n",
            "copying keras/layers/core.py -> build/lib/keras/layers\n",
            "copying keras/layers/merge.py -> build/lib/keras/layers\n",
            "copying keras/layers/advanced_activations.py -> build/lib/keras/layers\n",
            "copying keras/layers/convolutional_recurrent.py -> build/lib/keras/layers\n",
            "copying keras/layers/noise.py -> build/lib/keras/layers\n",
            "copying keras/layers/convolutional.py -> build/lib/keras/layers\n",
            "creating build/lib/keras/utils\n",
            "copying keras/utils/__init__.py -> build/lib/keras/utils\n",
            "copying keras/utils/io_utils.py -> build/lib/keras/utils\n",
            "copying keras/utils/data_utils.py -> build/lib/keras/utils\n",
            "copying keras/utils/multi_gpu_utils.py -> build/lib/keras/utils\n",
            "copying keras/utils/test_utils.py -> build/lib/keras/utils\n",
            "copying keras/utils/generic_utils.py -> build/lib/keras/utils\n",
            "copying keras/utils/np_utils.py -> build/lib/keras/utils\n",
            "copying keras/utils/vis_utils.py -> build/lib/keras/utils\n",
            "copying keras/utils/layer_utils.py -> build/lib/keras/utils\n",
            "copying keras/utils/conv_utils.py -> build/lib/keras/utils\n",
            "creating build/lib/keras/engine\n",
            "copying keras/engine/training_arrays.py -> build/lib/keras/engine\n",
            "copying keras/engine/training.py -> build/lib/keras/engine\n",
            "copying keras/engine/topology.py -> build/lib/keras/engine\n",
            "copying keras/engine/__init__.py -> build/lib/keras/engine\n",
            "copying keras/engine/training_utils.py -> build/lib/keras/engine\n",
            "copying keras/engine/sequential.py -> build/lib/keras/engine\n",
            "copying keras/engine/training_generator.py -> build/lib/keras/engine\n",
            "copying keras/engine/saving.py -> build/lib/keras/engine\n",
            "copying keras/engine/network.py -> build/lib/keras/engine\n",
            "copying keras/engine/input_layer.py -> build/lib/keras/engine\n",
            "copying keras/engine/base_layer.py -> build/lib/keras/engine\n",
            "creating build/lib/keras/backend\n",
            "copying keras/backend/cntk_backend.py -> build/lib/keras/backend\n",
            "copying keras/backend/theano_backend.py -> build/lib/keras/backend\n",
            "copying keras/backend/numpy_backend.py -> build/lib/keras/backend\n",
            "copying keras/backend/__init__.py -> build/lib/keras/backend\n",
            "copying keras/backend/common.py -> build/lib/keras/backend\n",
            "copying keras/backend/tensorflow_backend.py -> build/lib/keras/backend\n",
            "copying keras/backend/load_backend.py -> build/lib/keras/backend\n",
            "creating build/lib/keras/datasets\n",
            "copying keras/datasets/cifar10.py -> build/lib/keras/datasets\n",
            "copying keras/datasets/cifar.py -> build/lib/keras/datasets\n",
            "copying keras/datasets/fashion_mnist.py -> build/lib/keras/datasets\n",
            "copying keras/datasets/boston_housing.py -> build/lib/keras/datasets\n",
            "copying keras/datasets/__init__.py -> build/lib/keras/datasets\n",
            "copying keras/datasets/cifar100.py -> build/lib/keras/datasets\n",
            "copying keras/datasets/mnist.py -> build/lib/keras/datasets\n",
            "copying keras/datasets/imdb.py -> build/lib/keras/datasets\n",
            "copying keras/datasets/reuters.py -> build/lib/keras/datasets\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/keras\n",
            "creating build/bdist.linux-x86_64/egg/keras/preprocessing\n",
            "copying build/lib/keras/preprocessing/__init__.py -> build/bdist.linux-x86_64/egg/keras/preprocessing\n",
            "copying build/lib/keras/preprocessing/image.py -> build/bdist.linux-x86_64/egg/keras/preprocessing\n",
            "copying build/lib/keras/preprocessing/text.py -> build/bdist.linux-x86_64/egg/keras/preprocessing\n",
            "copying build/lib/keras/preprocessing/sequence.py -> build/bdist.linux-x86_64/egg/keras/preprocessing\n",
            "copying build/lib/keras/losses.py -> build/bdist.linux-x86_64/egg/keras\n",
            "creating build/bdist.linux-x86_64/egg/keras/backend\n",
            "copying build/lib/keras/backend/common.py -> build/bdist.linux-x86_64/egg/keras/backend\n",
            "copying build/lib/keras/backend/__init__.py -> build/bdist.linux-x86_64/egg/keras/backend\n",
            "copying build/lib/keras/backend/numpy_backend.py -> build/bdist.linux-x86_64/egg/keras/backend\n",
            "copying build/lib/keras/backend/theano_backend.py -> build/bdist.linux-x86_64/egg/keras/backend\n",
            "copying build/lib/keras/backend/load_backend.py -> build/bdist.linux-x86_64/egg/keras/backend\n",
            "copying build/lib/keras/backend/tensorflow_backend.py -> build/bdist.linux-x86_64/egg/keras/backend\n",
            "copying build/lib/keras/backend/cntk_backend.py -> build/bdist.linux-x86_64/egg/keras/backend\n",
            "creating build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/mobilenet_v2.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/__init__.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/mobilenet.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/inception_v3.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/imagenet_utils.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/densenet.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/inception_resnet_v2.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/xception.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/vgg16.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/mobilenetv2.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/resnet_v2.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/vgg19.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/nasnet.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/resnet50.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/resnet.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "copying build/lib/keras/applications/resnext.py -> build/bdist.linux-x86_64/egg/keras/applications\n",
            "creating build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/wrappers.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/convolutional_recurrent.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/normalization.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/embeddings.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/pooling.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/local.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/__init__.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/merge.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/noise.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/advanced_activations.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/recurrent.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/convolutional.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/cudnn_recurrent.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/layers/core.py -> build/bdist.linux-x86_64/egg/keras/layers\n",
            "copying build/lib/keras/activations.py -> build/bdist.linux-x86_64/egg/keras\n",
            "copying build/lib/keras/objectives.py -> build/bdist.linux-x86_64/egg/keras\n",
            "creating build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/training_arrays.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/base_layer.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/training_generator.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/topology.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/training_utils.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/__init__.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/input_layer.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/saving.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/network.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/training.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/engine/sequential.py -> build/bdist.linux-x86_64/egg/keras/engine\n",
            "copying build/lib/keras/callbacks.py -> build/bdist.linux-x86_64/egg/keras\n",
            "creating build/bdist.linux-x86_64/egg/keras/legacy\n",
            "copying build/lib/keras/legacy/interfaces.py -> build/bdist.linux-x86_64/egg/keras/legacy\n",
            "copying build/lib/keras/legacy/layers.py -> build/bdist.linux-x86_64/egg/keras/legacy\n",
            "copying build/lib/keras/legacy/__init__.py -> build/bdist.linux-x86_64/egg/keras/legacy\n",
            "copying build/lib/keras/initializers.py -> build/bdist.linux-x86_64/egg/keras\n",
            "copying build/lib/keras/metrics.py -> build/bdist.linux-x86_64/egg/keras\n",
            "copying build/lib/keras/optimizers.py -> build/bdist.linux-x86_64/egg/keras\n",
            "copying build/lib/keras/constraints.py -> build/bdist.linux-x86_64/egg/keras\n",
            "creating build/bdist.linux-x86_64/egg/keras/wrappers\n",
            "copying build/lib/keras/wrappers/scikit_learn.py -> build/bdist.linux-x86_64/egg/keras/wrappers\n",
            "copying build/lib/keras/wrappers/__init__.py -> build/bdist.linux-x86_64/egg/keras/wrappers\n",
            "creating build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/datasets/imdb.py -> build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/datasets/cifar10.py -> build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/datasets/mnist.py -> build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/datasets/cifar.py -> build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/datasets/cifar100.py -> build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/datasets/fashion_mnist.py -> build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/datasets/__init__.py -> build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/datasets/boston_housing.py -> build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/datasets/reuters.py -> build/bdist.linux-x86_64/egg/keras/datasets\n",
            "copying build/lib/keras/models.py -> build/bdist.linux-x86_64/egg/keras\n",
            "copying build/lib/keras/regularizers.py -> build/bdist.linux-x86_64/egg/keras\n",
            "copying build/lib/keras/__init__.py -> build/bdist.linux-x86_64/egg/keras\n",
            "creating build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/generic_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/layer_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/np_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/conv_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/io_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/data_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/vis_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/multi_gpu_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/__init__.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "copying build/lib/keras/utils/test_utils.py -> build/bdist.linux-x86_64/egg/keras/utils\n",
            "creating build/bdist.linux-x86_64/egg/docs\n",
            "copying build/lib/docs/autogen.py -> build/bdist.linux-x86_64/egg/docs\n",
            "copying build/lib/docs/structure.py -> build/bdist.linux-x86_64/egg/docs\n",
            "copying build/lib/docs/__init__.py -> build/bdist.linux-x86_64/egg/docs\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/preprocessing/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/preprocessing/image.py to image.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/preprocessing/text.py to text.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/preprocessing/sequence.py to sequence.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/losses.py to losses.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/backend/common.py to common.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/backend/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/backend/numpy_backend.py to numpy_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/backend/theano_backend.py to theano_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/backend/load_backend.py to load_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/backend/tensorflow_backend.py to tensorflow_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/backend/cntk_backend.py to cntk_backend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/mobilenet_v2.py to mobilenet_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/mobilenet.py to mobilenet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/inception_v3.py to inception_v3.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/imagenet_utils.py to imagenet_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/densenet.py to densenet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/inception_resnet_v2.py to inception_resnet_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/xception.py to xception.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/vgg16.py to vgg16.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/mobilenetv2.py to mobilenetv2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/resnet_v2.py to resnet_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/vgg19.py to vgg19.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/nasnet.py to nasnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/resnet50.py to resnet50.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/resnet.py to resnet.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/applications/resnext.py to resnext.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/wrappers.py to wrappers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/convolutional_recurrent.py to convolutional_recurrent.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/normalization.py to normalization.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/embeddings.py to embeddings.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/pooling.py to pooling.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/local.py to local.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/merge.py to merge.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/noise.py to noise.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/advanced_activations.py to advanced_activations.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/recurrent.py to recurrent.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/convolutional.py to convolutional.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/cudnn_recurrent.py to cudnn_recurrent.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/layers/core.py to core.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/activations.py to activations.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/objectives.py to objectives.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/training_arrays.py to training_arrays.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/base_layer.py to base_layer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/training_generator.py to training_generator.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/topology.py to topology.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/training_utils.py to training_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/input_layer.py to input_layer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/saving.py to saving.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/network.py to network.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/training.py to training.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/engine/sequential.py to sequential.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/callbacks.py to callbacks.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/legacy/interfaces.py to interfaces.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/legacy/layers.py to layers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/legacy/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/initializers.py to initializers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/metrics.py to metrics.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/optimizers.py to optimizers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/constraints.py to constraints.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/wrappers/scikit_learn.py to scikit_learn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/wrappers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/datasets/imdb.py to imdb.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/datasets/cifar10.py to cifar10.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/datasets/mnist.py to mnist.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/datasets/cifar.py to cifar.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/datasets/cifar100.py to cifar100.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/datasets/fashion_mnist.py to fashion_mnist.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/datasets/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/datasets/boston_housing.py to boston_housing.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/datasets/reuters.py to reuters.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/models.py to models.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/regularizers.py to regularizers.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/generic_utils.py to generic_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/layer_utils.py to layer_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/np_utils.py to np_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/conv_utils.py to conv_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/io_utils.py to io_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/data_utils.py to data_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/vis_utils.py to vis_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/multi_gpu_utils.py to multi_gpu_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras/utils/test_utils.py to test_utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/docs/autogen.py to autogen.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/docs/structure.py to structure.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/docs/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Keras.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Keras.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Keras.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Keras.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying Keras.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "docs.__pycache__.autogen.cpython-36: module references __file__\n",
            "docs.__pycache__.autogen.cpython-36: module MAY be using inspect.getsource\n",
            "creating dist\n",
            "creating 'dist/Keras-2.2.4-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing Keras-2.2.4-py3.6.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/Keras-2.2.4-py3.6.egg\n",
            "Extracting Keras-2.2.4-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding Keras 2.2.4 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Keras-2.2.4-py3.6.egg\n",
            "Processing dependencies for Keras==2.2.4\n",
            "Searching for Keras-Preprocessing==1.0.9\n",
            "Best match: Keras-Preprocessing 1.0.9\n",
            "Adding Keras-Preprocessing 1.0.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Applications==1.0.7\n",
            "Best match: Keras-Applications 1.0.7\n",
            "Adding Keras-Applications 1.0.7 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for h5py==2.8.0\n",
            "Best match: h5py 2.8.0\n",
            "Adding h5py 2.8.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for PyYAML==3.13\n",
            "Best match: PyYAML 3.13\n",
            "Adding PyYAML 3.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.11.0\n",
            "Best match: six 1.11.0\n",
            "Adding six 1.11.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.1.0\n",
            "Best match: scipy 1.1.0\n",
            "Adding scipy 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.14.6\n",
            "Best match: numpy 1.14.6\n",
            "Adding numpy 1.14.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for Keras==2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OOwHTEP-3gHQ"
      },
      "cell_type": "code",
      "source": [
        "from yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, scale_boxes\n",
        "from yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etxHjPiQRrAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "af04b53c-52f8-4d7e-d4b4-ab07246a45c3"
      },
      "cell_type": "code",
      "source": [
        "!pip install keras_resnet\n",
        "#import keras_resnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_resnet\n",
            "  Downloading https://files.pythonhosted.org/packages/05/46/ad0b2d1a05d9497bd80c98a2c3f4d8be38a4601ace69af72814f5fafd851/keras-resnet-0.1.0.tar.gz\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_resnet) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras_resnet) (1.11.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras_resnet) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras_resnet) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras_resnet) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras_resnet) (1.14.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras_resnet) (1.0.9)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras_resnet) (1.1.0)\n",
            "Building wheels for collected packages: keras-resnet\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/80/dd/ac/842235b63dddac12faa4b48ebe58b8944e8c2e57c2e38dddb6\n",
            "Successfully built keras-resnet\n",
            "Installing collected packages: keras-resnet\n",
            "Successfully installed keras-resnet-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "itbHwIkm3gHV"
      },
      "cell_type": "code",
      "source": [
        "from keras_retinanet.models.resnet import custom_objects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iGNf-OJoR2do"
      },
      "cell_type": "code",
      "source": [
        "#import keras_retinanet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ha3fWis5GZ2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2a70d562-f88c-457a-ff15-2c120f6cc9bd"
      },
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "#np.set_printoptions(threshold=np.nan)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OfSwmAgaGmOT",
        "outputId": "bd8c1691-dffa-472e-aa24-dbdee7eec37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "#load facenet trained model\n",
        "model=load_model('face_recog_verify.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:282: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CnELXUwK3gHj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b5e4c653-4f32-4602-a7a8-265c440a2c03"
      },
      "cell_type": "code",
      "source": [
        "#load flickr logos trained model\n",
        "logo_model = load_model('logo_model.h5', custom_objects=custom_objects)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:282: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cEMNT3T83gHo"
      },
      "cell_type": "code",
      "source": [
        "p = dict()\n",
        "#p['l2n'] = {0: 'adidas', 1: 'aldi', 2: 'apple', 3: 'becks', 4: 'bmw', 5: 'carlsberg', 6: 'chimay', 7: 'cocacola', 8: 'corona', 9: 'dhl', 10: 'erdinger', 11: 'esso', 12: 'fedex', 13: 'ferrari', 14: 'ford', 15: 'fosters', 16: 'google', 17: 'guiness', 18: 'heineken', 19: 'HP', 20: 'milka', 21: 'nvidia', 22: 'paulaner', 23: 'pepsi', 24: 'rittersport', 25: 'shell', 26: 'singha', 27: 'starbucks', 28: 'stellaartois', 29: 'texaco', 30: 'tsingtao', 31: 'ups'}\n",
        "p['l2n'] = {0: 'adidas', 7: 'cocacola'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bsj1xUFh3gHu"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):\n",
        "    \"\"\"Filters YOLO boxes by thresholding on object and class confidence.\n",
        "\n",
        "    Arguments:\n",
        "    box_confidence -- tensor of shape (19, 19, 5, 1)\n",
        "    boxes -- tensor of shape (19, 19, 5, 4)\n",
        "    box_class_probs -- tensor of shape (19, 19, 5, 80)\n",
        "    threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n",
        "\n",
        "    Returns:\n",
        "    scores -- tensor of shape (None,), containing the class probability score for selected boxes\n",
        "    boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes\n",
        "    classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes\n",
        "\n",
        "    Note: \"None\" is here because you don't know the exact number of selected boxes, as it depends on the threshold.\n",
        "    For example, the actual output size of scores would be (10,) if there are 10 boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute box scores\n",
        "    box_scores = box_confidence*box_class_probs\n",
        "\n",
        "    # Find the box_classes thanks to the max box_scores, keep track of the corresponding score\n",
        "    box_classes = K.argmax(box_scores,axis=-1)\n",
        "    box_class_scores = K.max(box_scores,axis=-1)\n",
        "\n",
        "    #Create a filtering mask based on \"box_class_scores\" by using \"threshold\". The mask should have the\n",
        "    # same dimension as box_class_scores, and be True for the boxes you want to keep (with probability >= threshold)\n",
        "    filtering_mask = (box_class_scores >= threshold)\n",
        "\n",
        "    #Apply the mask to scores, boxes and classes\n",
        "    scores = tf.boolean_mask(box_class_scores,filtering_mask)\n",
        "    boxes = tf.boolean_mask(boxes,filtering_mask)\n",
        "    classes = tf.boolean_mask(box_classes,filtering_mask)\n",
        "\n",
        "    return scores, boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v55lcNwd3gHy"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def iou(box1, box2):\n",
        "    \"\"\"Implement the intersection over union (IoU) between box1 and box2\n",
        "\n",
        "    Arguments:\n",
        "    box1 -- first box, list object with coordinates (x1, y1, x2, y2)\n",
        "    box2 -- second box, list object with coordinates (x1, y1, x2, y2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1 and box2. Calculate its Area.\n",
        "    xi1 = max(box1[0],box2[0])\n",
        "    yi1 = max(box1[1],box2[1])\n",
        "    xi2 = min(box1[2],box2[2])\n",
        "    yi2 = min(box1[3],box2[3])\n",
        "    inter_area = (yi2-yi1)*(xi2-xi1)\n",
        "\n",
        "    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
        "    box1_area = (box1[3]-box1[1])*(box1[2]-box1[0])\n",
        "    box2_area = (box2[3]-box2[1])*(box2[2]-box2[0])\n",
        "    union_area = box1_area+box2_area-inter_area\n",
        "\n",
        "    # compute the IoU\n",
        "    iou = inter_area/union_area\n",
        "\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8jzEJYmi3gH3"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
        "    \"\"\"\n",
        "    Applies Non-max suppression (NMS) to set of boxes\n",
        "\n",
        "    Arguments:\n",
        "    scores -- tensor of shape (None,), output of yolo_filter_boxes()\n",
        "    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)\n",
        "    classes -- tensor of shape (None,), output of yolo_filter_boxes()\n",
        "    max_boxes -- integer, maximum number of predicted boxes you'd like\n",
        "    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n",
        "\n",
        "    Returns:\n",
        "    scores -- tensor of shape (, None), predicted score for each box\n",
        "    boxes -- tensor of shape (4, None), predicted box coordinates\n",
        "    classes -- tensor of shape (, None), predicted class for each box\n",
        "\n",
        "    Note: The \"None\" dimension of the output tensors has obviously to be less than max_boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')     # tensor to be used in tf.image.non_max_suppression()\n",
        "    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor\n",
        "\n",
        "    nms_indices = tf.image.non_max_suppression(boxes,scores,max_boxes,iou_threshold)\n",
        "\n",
        "    # Use K.gather() to select only nms_indices from scores, boxes and classes\n",
        "    scores = K.gather(scores,nms_indices)\n",
        "    boxes = K.gather(boxes,nms_indices)\n",
        "    classes = K.gather(classes,nms_indices)\n",
        "\n",
        "    return scores, boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CxrkXvp63gH6"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.3, iou_threshold=.25):\n",
        "    \"\"\"\n",
        "    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes.\n",
        "    score_threshold=.6, iou_threshold=.5\n",
        "    Arguments:\n",
        "    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:\n",
        "                    box_confidence: tensor of shape (None, 19, 19, 5, 1)\n",
        "                    box_xy: tensor of shape (None, 19, 19, 5, 2)\n",
        "                    box_wh: tensor of shape (None, 19, 19, 5, 2)\n",
        "                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)\n",
        "    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)\n",
        "    max_boxes -- integer, maximum number of predicted boxes you'd like\n",
        "    score_threshold -- real value, if [ highest class probability score < threshold], then get rid of the corresponding box\n",
        "    iou_threshold -- real value, \"intersection over union\" threshold used for NMS filtering\n",
        "\n",
        "    Returns:\n",
        "    scores -- tensor of shape (None, ), predicted score for each box\n",
        "    boxes -- tensor of shape (None, 4), predicted box coordinates\n",
        "    classes -- tensor of shape (None,), predicted class for each box\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve outputs of the YOLO model (≈1 line)\n",
        "    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs\n",
        "\n",
        "    # Convert boxes to be ready for filtering functions\n",
        "    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
        "\n",
        "    # perform Score-filtering with a threshold of score_threshold\n",
        "    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, score_threshold)\n",
        "\n",
        "    # Scale boxes back to original image shape.\n",
        "    boxes = scale_boxes(boxes, image_shape)\n",
        "\n",
        "    # perform Non-max suppression with a threshold of iou_threshold\n",
        "    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold)\n",
        "\n",
        "    return scores, boxes, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7ub2crp3gH-"
      },
      "cell_type": "code",
      "source": [
        "def draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors):\n",
        "\n",
        "    font = ImageFont.truetype(font='font/FiraMono-Medium.otf',size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
        "    thickness = (image.size[0] + image.size[1]) // 300\n",
        "\n",
        "    for i, c in reversed(list(enumerate(out_classes))):\n",
        "        predicted_class = class_names[c]\n",
        "        if predicted_class != \"sports ball\":\n",
        "            continue\n",
        "        box = out_boxes[i]\n",
        "        score = out_scores[i]\n",
        "\n",
        "        label = '{} {:.2f}'.format(predicted_class, score)\n",
        "\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        label_size = draw.textsize(label, font)\n",
        "\n",
        "        top, left, bottom, right = box\n",
        "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
        "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
        "        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
        "        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
        "        #print(label, (left, top), (right, bottom))\n",
        "\n",
        "        if top - label_size[1] >= 0:\n",
        "            text_origin = np.array([left, top - label_size[1]])\n",
        "        else:\n",
        "            text_origin = np.array([left, top + 1])\n",
        "\n",
        "        # My kingdom for a good redistributable image drawing library.\n",
        "        for i in range(thickness):\n",
        "            draw.rectangle([left + i, top + i, right - i, bottom - i], outline=colors[c])\n",
        "        draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=colors[c])\n",
        "        draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "        del draw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DcRTjvzW3gIC"
      },
      "cell_type": "code",
      "source": [
        "def predict2(sess, image_file):\n",
        "    \"\"\"\n",
        "    Runs the graph stored in \"sess\" to predict boxes for \"image_file\". Prints and plots the preditions.\n",
        "\n",
        "    Arguments:\n",
        "    sess -- your tensorflow/Keras session containing the YOLO graph\n",
        "    image_file -- name of an image stored in the \"images\" folder.\n",
        "\n",
        "    Returns:\n",
        "    out_scores -- tensor of shape (None, ), scores of the predicted boxes\n",
        "    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes\n",
        "    out_classes -- tensor of shape (None, ), class index of the predicted boxes\n",
        "\n",
        "    Note: \"None\" actually represents the number of predicted boxes, it varies between 0 and max_boxes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess your image\n",
        "    #image, image_data = preprocess_image(\"images/\" + image_file, model_image_size = (608, 608))\n",
        "    image=image_file\n",
        "    resized_image_disp = image.resize((1280,720), Image.BICUBIC)\n",
        "    #model_image_size=(608,608)\n",
        "    resized_image = image.resize((608,608), Image.BICUBIC)\n",
        "    #resized_image = image.resize(tuple(reversed((608,608))), Image.BICUBIC)\n",
        "    #resized_image = image.resize((720,1280), Image.BICUBIC)\n",
        "    #resized_image = scipy.misc.imresize(image, size=(608,608),interp='bicubic')\n",
        "    image_data = np.array(resized_image, dtype='float32')\n",
        "    #image_disp = image_data\n",
        "    image_data /= 255.\n",
        "    image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
        "    #image_data=image_file\n",
        "\n",
        "    # Run the session with the correct tensors and choose the correct placeholders in the feed_dict.\n",
        "    # You'll need to use feed_dict={yolo_model.input: ... , K.learning_phase(): 0})\n",
        "    ### START CODE HERE ### (≈ 1 line)\n",
        "    out_scores, out_boxes, out_classes = sess.run([scores,boxes,classes],feed_dict={yolo_model.input:image_data,K.learning_phase(): 0})\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print predictions info\n",
        "    #print('Found {} boxes for {}'.format(len(out_boxes), \"image_file\"))\n",
        "    # Generate colors for drawing bounding boxes.\n",
        "    colors = generate_colors(class_names)\n",
        "    # Draw bounding boxes on the image file\n",
        "    draw_boxes(resized_image_disp, out_scores, out_boxes, out_classes, class_names, colors)\n",
        "    # Save the predicted bounding box on the image\n",
        "    #image.save(os.path.join(\"out\", image_file), quality=90)\n",
        "    # Display the results in the notebook\n",
        "    #output_image = scipy.misc.imread(os.path.join(\"out\", image_file))\n",
        "    #imshow(image)\n",
        "\n",
        "    #cv2.imshow('frame',cv2.cvtColor(np.array(image),cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    #return out_scores, out_boxes, out_classes\n",
        "    return resized_image_disp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wIqyzdZl3gII"
      },
      "cell_type": "code",
      "source": [
        "sess = K.get_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BElsjHm33gIL"
      },
      "cell_type": "code",
      "source": [
        "class_names = read_classes(\"model_data/coco_classes.txt\")\n",
        "anchors = read_anchors(\"model_data/yolo_anchors.txt\")\n",
        "image_shape = (720., 1280.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2gp0wzTa3gIS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6592366f-c9b2-4ba1-8281-8f76fbeb6612"
      },
      "cell_type": "code",
      "source": [
        "yolo_model = load_model(\"model_data/yolo.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:282: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4uYnQMt33gIY"
      },
      "cell_type": "code",
      "source": [
        "#print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NopHFSJd3gId"
      },
      "cell_type": "code",
      "source": [
        "yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esJltLJF3gIh"
      },
      "cell_type": "code",
      "source": [
        "scores, boxes, classes = yolo_eval(yolo_outputs, image_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEY7eiCQG849"
      },
      "cell_type": "code",
      "source": [
        "def img_to_encoding(image_path, model):\n",
        "    img1 = cv2.imread(image_path, 1)\n",
        "    img = img1[...,::-1]\n",
        "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
        "    x_train = np.array([img])\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XbR9DdMBXm5p"
      },
      "cell_type": "code",
      "source": [
        "database = {}\n",
        "'''database[\"ahmed_musa\"] = img_to_encoding(\"players/ahmed_musa.jpg\", model)\n",
        "database[\"benjamin_pavard\"] = img_to_encoding(\"players/benjamin_pavard.jpg\", model)\n",
        "database[\"cristiano_ronaldo\"] = img_to_encoding(\"players/cristiano_ronaldo.jpg\", model)\n",
        "database[\"denis_cheryshev\"] = img_to_encoding(\"players/denis_cheryshev.jpg\", model)\n",
        "database[\"juan_quintero\"] = img_to_encoding(\"players/juan_quintero.jpg\", model)\n",
        "database[\"lionel_messi\"] = img_to_encoding(\"players/lionel_messi.jpg\", model)\n",
        "database[\"luka_modric\"] = img_to_encoding(\"players/luka_modric.jpg\", model)\n",
        "database[\"nacer_chadli\"] = img_to_encoding(\"players/nacer_chadli.jpg\", model)\n",
        "database[\"ricardo_quaresma\"] = img_to_encoding(\"players/ricardo_quaresma.jpg\", model)\n",
        "database[\"toni_kroos\"] = img_to_encoding(\"players/toni_kroos.jpg\", model)'''\n",
        "database[\"ahmed_musa\"] = img_to_encoding(\"players/frame612.jpg\", model)\n",
        "database[\"ahmed_musa_2\"] = img_to_encoding(\"players/frame673.jpg\", model)\n",
        "database[\"benjamin_pavard\"] = img_to_encoding(\"players/frame2026.jpg\", model)\n",
        "database[\"benjamin_pavard_2\"] = img_to_encoding(\"players/frame2126.jpg\", model)\n",
        "database[\"cristiano_ronaldo\"] = img_to_encoding(\"players/frame1424.jpg\", model)\n",
        "database[\"cristiano_ronaldo_2\"] = img_to_encoding(\"players/frame1447.jpg\", model)\n",
        "database[\"cristiano_ronaldo_3\"] = img_to_encoding(\"players/frame1488.jpg\", model)\n",
        "database[\"denis_cheryshev\"] = img_to_encoding(\"players/frame961.jpg\", model)\n",
        "database[\"denis_cheryshev_2\"] = img_to_encoding(\"players/frame1093.jpg\", model)\n",
        "database[\"juan_quintero\"] = img_to_encoding(\"players/frame1735.jpg\", model)\n",
        "database[\"juan_quintero_2\"] = img_to_encoding(\"players/frame1860.jpg\", model)\n",
        "database[\"lionel_messi\"] = img_to_encoding(\"players/frame1166.jpg\", model)\n",
        "database[\"lionel_messi_2\"] = img_to_encoding(\"players/frame1222.jpg\", model)\n",
        "database[\"lionel_messi_3\"] = img_to_encoding(\"players/frame1251.jpg\", model)\n",
        "database[\"luka_modric\"] = img_to_encoding(\"players/frame1538.jpg\", model)\n",
        "database[\"luka_modric_2\"] = img_to_encoding(\"players/frame1650.jpg\", model)\n",
        "database[\"nacer_chadli\"] = img_to_encoding(\"players/frame762.jpg\", model)\n",
        "database[\"ricardo_quaresma\"] = img_to_encoding(\"players/frame431.jpg\", model)\n",
        "database[\"ricardo_quaresma_2\"] = img_to_encoding(\"players/frame509.jpg\", model)\n",
        "database[\"toni_kroos\"] = img_to_encoding(\"players/frame19.jpg\", model)\n",
        "database[\"toni_kroos_2\"] = img_to_encoding(\"players/frame184.jpg\", model)\n",
        "database[\"toni_kroos_3\"] = img_to_encoding(\"players/frame91.jpg\", model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yldw6PvqHXAS"
      },
      "cell_type": "code",
      "source": [
        "def who_is_it(image_path, database, model):\n",
        "    \"\"\"\n",
        "\n",
        "    Arguments:\n",
        "    image_path -- path to an image\n",
        "    database -- database containing image encodings along with the name of the person on the image\n",
        "    model -- your Inception model instance in Keras\n",
        "\n",
        "    Returns:\n",
        "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
        "    identity -- string, the name prediction for the person on image_path\n",
        "    \"\"\"\n",
        "\n",
        "    ##Compute the target \"encoding\" for the image.\n",
        "    #encoding = img_to_encoding(image_path,model)\n",
        "    img = image_path[...,::-1]\n",
        "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
        "    x_train = np.array([img])\n",
        "    encoding = model.predict_on_batch(x_train)\n",
        "\n",
        "    ##Find the closest encoding ##\n",
        "\n",
        "    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n",
        "    min_dist = 100\n",
        "\n",
        "    # Loop over the database dictionary's names and encodings.\n",
        "    for (name, db_enc) in database.items():\n",
        "\n",
        "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line)\n",
        "        dist = np.linalg.norm(encoding-db_enc)\n",
        "\n",
        "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
        "        if dist<min_dist:\n",
        "            min_dist = dist\n",
        "            identity = name\n",
        "\n",
        "    if min_dist > 0.7:\n",
        "        #print(\"Not in the database.\")\n",
        "        pass\n",
        "    else:\n",
        "        #print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
        "        pass\n",
        "\n",
        "    return min_dist, identity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XAZoYd813gIv"
      },
      "cell_type": "code",
      "source": [
        "def resize_image(img, min_side=600, max_side=1024):\n",
        "    (rows, cols, _) = img.shape\n",
        "\n",
        "    smallest_side = min(rows, cols)\n",
        "\n",
        "    # rescale the image so the smallest side is min_side\n",
        "    scale = min_side / smallest_side\n",
        "\n",
        "    # check if the largest side is now greater than max_side, wich can happen\n",
        "    # when images have a large aspect ratio\n",
        "    largest_side = max(rows, cols)\n",
        "    if largest_side * scale > max_side:\n",
        "        scale = max_side / largest_side\n",
        "\n",
        "    # resize the image with the computed scale\n",
        "    img = cv2.resize(img, None, fx=scale, fy=scale)\n",
        "\n",
        "    return img, scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PIsTdIT33gIy"
      },
      "cell_type": "code",
      "source": [
        "def detectlogo(prediction_model,image,p):\n",
        "    draw_logo = image.copy()\n",
        "    image, scale = resize_image(image)\n",
        "    _, _, detections_logo = prediction_model.predict_on_batch(np.expand_dims(image, axis=0))\n",
        "    predicted_labels = np.argmax(detections_logo[0, :, 4:], axis=1)\n",
        "    logo_scores = detections_logo[0, np.arange(detections_logo.shape[1]), 4 + predicted_labels]\n",
        "    detections_logo[:, :4] /= scale\n",
        "\n",
        "    for idx, (label, logo_score) in enumerate(zip(predicted_labels, logo_scores)):\n",
        "        if logo_score < 0.01 or (label != 0 and label != 7):\n",
        "            continue\n",
        "        b = detections_logo[0, idx, :4].astype(int)\n",
        "        #print(label)\n",
        "\n",
        "        cv2.rectangle(draw_logo, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 3)\n",
        "        caption = \"{} {:.3f}\".format(p['l2n'][label], logo_score)\n",
        "        cv2.putText(draw_logo, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
        "        cv2.putText(draw_logo, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1.5, (255, 255, 255), 2)\n",
        "    return draw_logo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UISsA1v3eHDf"
      },
      "cell_type": "code",
      "source": [
        "#who_is_it(\"players/find1.jpg\", database, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-yTWlexaMn7A",
        "outputId": "33b6c72b-6290-4c73-fbc0-7f170d040af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2887
        }
      },
      "cell_type": "code",
      "source": [
        "net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt.txt\", \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "#cap = cv2.VideoCapture(0)\n",
        "#read youtueb video\n",
        "cap = cv2.VideoCapture('top_10.mp4')\n",
        "output_video_name='detected_video.mp4'\n",
        "frameId = cap.get(1)\n",
        "count = 0\n",
        "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "out = cv2.VideoWriter(output_video_name,fourcc, 25.0, (640,360))\n",
        "while(True):\n",
        "  #tic=time.time()\n",
        "  #time.sleep(1)\n",
        "  cap.set(1,frameId+0)\n",
        "  ret, frame = cap.read(1)\n",
        "  frameId = cap.get(1)\n",
        "  (h, w) = frame.shape[:2]\n",
        "  blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,(300, 300), (104.0, 177.0, 123.0))\n",
        "  # pass the blob through the network and obtain the detections and\n",
        "  # predictions\n",
        "  net.setInput(blob)\n",
        "  detections = net.forward()\n",
        "  count_frame=0\n",
        "  # loop over the detections\n",
        "  skip=0\n",
        "  flag_d=0\n",
        "  resize_failure=0\n",
        "  min_d=0.7\n",
        "  for i in range(0, detections.shape[2]):\n",
        "    #skip=0\n",
        "    # extract the confidence (i.e., probability) associated with the\n",
        "    # prediction\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "    # filter out weak detections by ensuring the `confidence` is\n",
        "    # greater than the minimum confidence\n",
        "    if confidence < 0.25:\n",
        "      #print(confidence)\n",
        "      if i == (detections.shape[2]-1):\n",
        "        skip=1\n",
        "        #break\n",
        "      #print(\"skipping\")\n",
        "      continue\n",
        "\n",
        "    # compute the (x, y)-coordinates of the bounding box for the\n",
        "    # object\n",
        "    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "    (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "    #print(startX, startY, endX, endY)\n",
        "\n",
        "    # draw the bounding box of the face along with the associated\n",
        "    # probability\n",
        "    y = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "    crop_img = frame[startY:endY, startX:endX]\n",
        "    im_x,im_y,im_z=np.shape(crop_img)\n",
        "    try:\n",
        "      crop_r=cv2.resize(crop_img, (96,96))\n",
        "    except:\n",
        "      resize_failure=1\n",
        "      #print(\"breaking\")\n",
        "      break\n",
        "    #detect player\n",
        "    #cv2.imwrite(\"detected_player_face/frame%d.jpg\" % count,crop_r)\n",
        "    #count += 1\n",
        "    distance,name=who_is_it(crop_r, database, model)\n",
        "    if distance < min_d:\n",
        "      flag_d=1\n",
        "      new_startX=startX\n",
        "      new_startY=startY\n",
        "      new_endX=endX\n",
        "      new_endY=endY\n",
        "      new_name=name\n",
        "      new_distance=distance\n",
        "      new_y=y\n",
        "      min_d=distance\n",
        "      #print(min_d)\n",
        "  if flag_d == 1:\n",
        "    #print(\"skip\")\n",
        "    startX=new_startX\n",
        "    startY=new_startY\n",
        "    endX=new_endX\n",
        "    endY=new_endY\n",
        "    name=new_name\n",
        "    y=new_y\n",
        "    distance=new_distance\n",
        "    if (y-12)<=0:\n",
        "      box_start_y=endY+5\n",
        "      box_end_y=box_start_y+45\n",
        "      text_y=endY+12+20\n",
        "      ##line_y=endY+3+12*i+15\n",
        "    else:\n",
        "      ##line_y=y-3-12*i\n",
        "      box_start_y=y+5\n",
        "      box_end_y=y-12-15\n",
        "      text_y=y-12\n",
        "    cv2.rectangle(frame, (startX, startY), (endX, endY),(0, 150, 0), 2)\n",
        "    overlay=frame.copy()\n",
        "    cv2.rectangle(overlay, (startX, box_start_y), (startX+110, box_end_y),(0, 150, 0), -2)\n",
        "    cv2.addWeighted(overlay, 0.5, frame, 0.5,0, frame)\n",
        "    cv2.putText(frame, \"{0}\".format(name), (startX+5, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (200, 0, 0), 1)\n",
        "  cv2_im=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "  #logo_image=frame\n",
        "  pil_im=Image.fromarray(cv2_im)\n",
        "  #detect ball\n",
        "  ball = predict2(sess, pil_im)\n",
        "  #detect logo\n",
        "  ball_player=cv2.cvtColor(np.array(ball),cv2.COLOR_RGB2BGR)\n",
        "  #ball_player=logo_image\n",
        "  logo_image=detectlogo(logo_model,ball_player,p)\n",
        "  ##logo_image=cv2.resize(ball_player,(640,480))\n",
        "  logo_image=cv2.resize(logo_image,(640,360))\n",
        "  #cv2.imwrite(\"sample_image.jpg\",logo_image)\n",
        "  #print(np.shape(logo_image))\n",
        "  out.write(logo_image)\n",
        "  if count%50 == 0:\n",
        "    print(count)\n",
        "    #break\n",
        "  count += 1\n",
        "  #cv2.imshow('ball_player_logo',logo_image)\n",
        "  #cv2.imshow(\"fifa\",frame)\n",
        "  #key = cv2.waitKey(1) & 0xFF\n",
        "  #if key == ord(\"q\"):\n",
        "  #  break\n",
        "cap.release()\n",
        "out.release()\n",
        "#cv2.destroyAllWindows()\n",
        "\n",
        "    #cv2.imwrite(\"players/frame%d.jpg\" % count,crop_r)\n",
        "    #count += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "2200\n",
            "2250\n",
            "2300\n",
            "2350\n",
            "2400\n",
            "2450\n",
            "2500\n",
            "2550\n",
            "2600\n",
            "2650\n",
            "2700\n",
            "2750\n",
            "2800\n",
            "2850\n",
            "2900\n",
            "2950\n",
            "3000\n",
            "3050\n",
            "3100\n",
            "3150\n",
            "3200\n",
            "3250\n",
            "3300\n",
            "3350\n",
            "3400\n",
            "3450\n",
            "3500\n",
            "3550\n",
            "3600\n",
            "3650\n",
            "3700\n",
            "3750\n",
            "3800\n",
            "3850\n",
            "3900\n",
            "3950\n",
            "4000\n",
            "4050\n",
            "4100\n",
            "4150\n",
            "4200\n",
            "4250\n",
            "4300\n",
            "4350\n",
            "4400\n",
            "4450\n",
            "4500\n",
            "4550\n",
            "4600\n",
            "4650\n",
            "4700\n",
            "4750\n",
            "4800\n",
            "4850\n",
            "4900\n",
            "4950\n",
            "5000\n",
            "5050\n",
            "5100\n",
            "5150\n",
            "5200\n",
            "5250\n",
            "5300\n",
            "5350\n",
            "5400\n",
            "5450\n",
            "5500\n",
            "5550\n",
            "5600\n",
            "5650\n",
            "5700\n",
            "5750\n",
            "5800\n",
            "5850\n",
            "5900\n",
            "5950\n",
            "6000\n",
            "6050\n",
            "6100\n",
            "6150\n",
            "6200\n",
            "6250\n",
            "6300\n",
            "6350\n",
            "6400\n",
            "6450\n",
            "6500\n",
            "6550\n",
            "6600\n",
            "6650\n",
            "6700\n",
            "6750\n",
            "6800\n",
            "6850\n",
            "6900\n",
            "6950\n",
            "7000\n",
            "7050\n",
            "7100\n",
            "7150\n",
            "7200\n",
            "7250\n",
            "7300\n",
            "7350\n",
            "7400\n",
            "7450\n",
            "7500\n",
            "7550\n",
            "7600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2560b897716f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mframeId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m104.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m177.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m123.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# pass the blob through the network and obtain the detections and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "p_NMf8o1Lg0b"
      },
      "cell_type": "code",
      "source": [
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QoDlofDETtMa"
      },
      "cell_type": "code",
      "source": [
        "cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zAqpNEGpT2mI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "986b574e-95e3-4a30-c627-59a25b982e25"
      },
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/10/330cbc8e63d072d40413f4d470444a6a1e8c8c6a80b2a4ac302d1252ca1b/ffmpeg_python-0.1.17-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.1.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ffmpeg"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LnxordXJUb4F"
      },
      "cell_type": "code",
      "source": [
        "import ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8RvhmtSlXV03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10348
        },
        "outputId": "c3229a0b-a236-46c8-8188-b75a80e9c95e"
      },
      "cell_type": "code",
      "source": [
        "help(ffmpeg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on package ffmpeg:\n",
            "\n",
            "NAME\n",
            "    ffmpeg - module(name[, doc])\n",
            "\n",
            "DESCRIPTION\n",
            "    Create a module object.\n",
            "    The name must be a string; the optional doc argument can have any type.\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    _ffmpeg\n",
            "    _filters\n",
            "    _probe\n",
            "    _run\n",
            "    _utils\n",
            "    _view\n",
            "    dag\n",
            "    nodes\n",
            "\n",
            "CLASSES\n",
            "    builtins.Exception(builtins.BaseException)\n",
            "        ffmpeg._run.Error\n",
            "    \n",
            "    class Error(builtins.Exception)\n",
            "     |  Common base class for all non-exit exceptions.\n",
            "     |  \n",
            "     |  Method resolution order:\n",
            "     |      Error\n",
            "     |      builtins.Exception\n",
            "     |      builtins.BaseException\n",
            "     |      builtins.object\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, cmd, stdout, stderr)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from builtins.Exception:\n",
            "     |  \n",
            "     |  __new__(*args, **kwargs) from builtins.type\n",
            "     |      Create and return a new object.  See help(type) for accurate signature.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __delattr__(self, name, /)\n",
            "     |      Implement delattr(self, name).\n",
            "     |  \n",
            "     |  __getattribute__(self, name, /)\n",
            "     |      Return getattr(self, name).\n",
            "     |  \n",
            "     |  __reduce__(...)\n",
            "     |      helper for pickle\n",
            "     |  \n",
            "     |  __repr__(self, /)\n",
            "     |      Return repr(self).\n",
            "     |  \n",
            "     |  __setattr__(self, name, value, /)\n",
            "     |      Implement setattr(self, name, value).\n",
            "     |  \n",
            "     |  __setstate__(...)\n",
            "     |  \n",
            "     |  __str__(self, /)\n",
            "     |      Return str(self).\n",
            "     |  \n",
            "     |  with_traceback(...)\n",
            "     |      Exception.with_traceback(tb) --\n",
            "     |      set self.__traceback__ to tb and return self.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from builtins.BaseException:\n",
            "     |  \n",
            "     |  __cause__\n",
            "     |      exception cause\n",
            "     |  \n",
            "     |  __context__\n",
            "     |      exception context\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |  \n",
            "     |  __suppress_context__\n",
            "     |  \n",
            "     |  __traceback__\n",
            "     |  \n",
            "     |  args\n",
            "\n",
            "FUNCTIONS\n",
            "    colorchannelmixer(stream, *args, **kwargs)\n",
            "        Adjust video input frames by re-mixing color channels.\n",
            "        \n",
            "        Official documentation: `colorchannelmixer <https://ffmpeg.org/ffmpeg-filters.html#colorchannelmixer>`__\n",
            "    \n",
            "    compile(stream_spec, cmd='ffmpeg', overwrite_output=False)\n",
            "        Build command-line for invoking ffmpeg.\n",
            "        \n",
            "        The :meth:`run` function uses this to build the commnad line\n",
            "        arguments and should work in most cases, but calling this function\n",
            "        directly is useful for debugging or if you need to invoke ffmpeg\n",
            "        manually for whatever reason.\n",
            "        \n",
            "        This is the same as calling :meth:`get_args` except that it also\n",
            "        includes the ``ffmpeg`` command as the first argument.\n",
            "    \n",
            "    concat(*streams, **kwargs)\n",
            "        Concatenate audio and video streams, joining them together one after the other.\n",
            "        \n",
            "        The filter works on segments of synchronized video and audio streams. All segments must have the same number of\n",
            "        streams of each type, and that will also be the number of streams at output.\n",
            "        \n",
            "        Args:\n",
            "            unsafe: Activate unsafe mode: do not fail if segments have a different format.\n",
            "        \n",
            "        Related streams do not always have exactly the same duration, for various reasons including codec frame size or\n",
            "        sloppy authoring. For that reason, related synchronized streams (e.g. a video and its audio track) should be\n",
            "        concatenated at once. The concat filter will use the duration of the longest stream in each segment (except the\n",
            "        last one), and if necessary pad shorter audio streams with silence.\n",
            "        \n",
            "        For this filter to work correctly, all segments must start at timestamp 0.\n",
            "        \n",
            "        All corresponding streams must have the same parameters in all segments; the filtering system will automatically\n",
            "        select a common pixel format for video streams, and a common sample format, sample rate and channel layout for\n",
            "        audio streams, but other settings, such as resolution, must be converted explicitly by the user.\n",
            "        \n",
            "        Different frame rates are acceptable but will result in variable frame rate at output; be sure to configure the\n",
            "        output file to handle it.\n",
            "        \n",
            "        Official documentation: `concat <https://ffmpeg.org/ffmpeg-filters.html#concat>`__\n",
            "    \n",
            "    crop(stream, x, y, width, height, **kwargs)\n",
            "        Crop the input video.\n",
            "        \n",
            "        Args:\n",
            "            x: The horizontal position, in the input video, of the left edge of\n",
            "               the output video.\n",
            "            y: The vertical position, in the input video, of the top edge of the\n",
            "               output video.\n",
            "            width: The width of the output video. Must be greater than 0.\n",
            "            heigth: The height of the output video. Must be greater than 0.\n",
            "        \n",
            "        Official documentation: `crop <https://ffmpeg.org/ffmpeg-filters.html#crop>`__\n",
            "    \n",
            "    drawbox(stream, x, y, width, height, color, thickness=None, **kwargs)\n",
            "        Draw a colored box on the input image.\n",
            "        \n",
            "        Args:\n",
            "            x: The expression which specifies the top left corner x coordinate of the box. It defaults to 0.\n",
            "            y: The expression which specifies the top left corner y coordinate of the box. It defaults to 0.\n",
            "            width: Specify the width of the box; if 0 interpreted as the input width. It defaults to 0.\n",
            "            heigth: Specify the height of the box; if 0 interpreted as the input height. It defaults to 0.\n",
            "            color: Specify the color of the box to write. For the general syntax of this option, check the \"Color\" section\n",
            "                in the ffmpeg-utils manual. If the special value invert is used, the box edge color is the same as the\n",
            "                video with inverted luma.\n",
            "            thickness: The expression which sets the thickness of the box edge. Default value is 3.\n",
            "            w: Alias for ``width``.\n",
            "            h: Alias for ``height``.\n",
            "            c: Alias for ``color``.\n",
            "            t: Alias for ``thickness``.\n",
            "        \n",
            "        Official documentation: `drawbox <https://ffmpeg.org/ffmpeg-filters.html#drawbox>`__\n",
            "    \n",
            "    drawtext(stream, text=None, x=0, y=0, escape_text=True, **kwargs)\n",
            "        Draw a text string or text from a specified file on top of a video, using the libfreetype library.\n",
            "        \n",
            "        To enable compilation of this filter, you need to configure FFmpeg with ``--enable-libfreetype``. To enable default\n",
            "        font fallback and the font option you need to configure FFmpeg with ``--enable-libfontconfig``. To enable the\n",
            "        text_shaping option, you need to configure FFmpeg with ``--enable-libfribidi``.\n",
            "        \n",
            "        Args:\n",
            "            box: Used to draw a box around text using the background color. The value must be either 1 (enable) or 0\n",
            "                (disable). The default value of box is 0.\n",
            "            boxborderw: Set the width of the border to be drawn around the box using boxcolor. The default value of\n",
            "                boxborderw is 0.\n",
            "            boxcolor: The color to be used for drawing box around text. For the syntax of this option, check the \"Color\"\n",
            "                section in the ffmpeg-utils manual.  The default value of boxcolor is \"white\".\n",
            "            line_spacing: Set the line spacing in pixels of the border to be drawn around the box using box. The default\n",
            "                value of line_spacing is 0.\n",
            "            borderw: Set the width of the border to be drawn around the text using bordercolor. The default value of\n",
            "                borderw is 0.\n",
            "            bordercolor: Set the color to be used for drawing border around text. For the syntax of this option, check the\n",
            "                \"Color\" section in the ffmpeg-utils manual.  The default value of bordercolor is \"black\".\n",
            "            expansion: Select how the text is expanded. Can be either none, strftime (deprecated) or normal (default). See\n",
            "                the Text expansion section below for details.\n",
            "            basetime: Set a start time for the count. Value is in microseconds. Only applied in the deprecated strftime\n",
            "                expansion mode. To emulate in normal expansion mode use the pts function, supplying the start time (in\n",
            "                seconds) as the second argument.\n",
            "            fix_bounds: If true, check and fix text coords to avoid clipping.\n",
            "            fontcolor: The color to be used for drawing fonts. For the syntax of this option, check the \"Color\" section in\n",
            "                the ffmpeg-utils manual.  The default value of fontcolor is \"black\".\n",
            "            fontcolor_expr: String which is expanded the same way as text to obtain dynamic fontcolor value. By default\n",
            "                this option has empty value and is not processed. When this option is set, it overrides fontcolor option.\n",
            "            font: The font family to be used for drawing text. By default Sans.\n",
            "            fontfile: The font file to be used for drawing text. The path must be included. This parameter is mandatory if\n",
            "                the fontconfig support is disabled.\n",
            "            alpha: Draw the text applying alpha blending. The value can be a number between 0.0 and 1.0. The expression\n",
            "                accepts the same variables x, y as well. The default value is 1. Please see fontcolor_expr.\n",
            "            fontsize: The font size to be used for drawing text. The default value of fontsize is 16.\n",
            "            text_shaping: If set to 1, attempt to shape the text (for example, reverse the order of right-to-left text and\n",
            "                join Arabic characters) before drawing it. Otherwise, just draw the text exactly as given. By default 1 (if\n",
            "                supported).\n",
            "            ft_load_flags: The flags to be used for loading the fonts. The flags map the corresponding flags supported by\n",
            "                libfreetype, and are a combination of the following values:\n",
            "        \n",
            "                * ``default``\n",
            "                * ``no_scale``\n",
            "                * ``no_hinting``\n",
            "                * ``render``\n",
            "                * ``no_bitmap``\n",
            "                * ``vertical_layout``\n",
            "                * ``force_autohint``\n",
            "                * ``crop_bitmap``\n",
            "                * ``pedantic``\n",
            "                * ``ignore_global_advance_width``\n",
            "                * ``no_recurse``\n",
            "                * ``ignore_transform``\n",
            "                * ``monochrome``\n",
            "                * ``linear_design``\n",
            "                * ``no_autohint``\n",
            "        \n",
            "                Default value is \"default\".  For more information consult the documentation for the FT_LOAD_* libfreetype\n",
            "                flags.\n",
            "            shadowcolor: The color to be used for drawing a shadow behind the drawn text. For the syntax of this option,\n",
            "                check the \"Color\" section in the ffmpeg-utils manual.  The default value of shadowcolor is \"black\".\n",
            "            shadowx: The x offset for the text shadow position with respect to the position of the text. It can be either\n",
            "                positive or negative values. The default value is \"0\".\n",
            "            shadowy: The y offset for the text shadow position with respect to the position of the text. It can be either\n",
            "                positive or negative values. The default value is \"0\".\n",
            "            start_number: The starting frame number for the n/frame_num variable. The default value is \"0\".\n",
            "            tabsize: The size in number of spaces to use for rendering the tab. Default value is 4.\n",
            "            timecode: Set the initial timecode representation in \"hh:mm:ss[:;.]ff\" format. It can be used with or without\n",
            "                text parameter. timecode_rate option must be specified.\n",
            "            rate: Set the timecode frame rate (timecode only).\n",
            "            timecode_rate: Alias for ``rate``.\n",
            "            r: Alias for ``rate``.\n",
            "            tc24hmax: If set to 1, the output of the timecode option will wrap around at 24 hours. Default is 0 (disabled).\n",
            "            text: The text string to be drawn. The text must be a sequence of UTF-8 encoded characters. This parameter is\n",
            "                mandatory if no file is specified with the parameter textfile.\n",
            "            textfile: A text file containing text to be drawn. The text must be a sequence of UTF-8 encoded characters.\n",
            "                This parameter is mandatory if no text string is specified with the parameter text.  If both text and\n",
            "                textfile are specified, an error is thrown.\n",
            "            reload: If set to 1, the textfile will be reloaded before each frame. Be sure to update it atomically, or it\n",
            "                may be read partially, or even fail.\n",
            "            x: The expression which specifies the offset where text will be drawn within the video frame. It is relative to\n",
            "                the left border of the output image. The default value is \"0\".\n",
            "            y: The expression which specifies the offset where text will be drawn within the video frame. It is relative to\n",
            "                the top border of the output image. The default value is \"0\".  See below for the list of accepted constants\n",
            "                and functions.\n",
            "        \n",
            "        Expression constants:\n",
            "            The parameters for x and y are expressions containing the following constants and functions:\n",
            "             - dar: input display aspect ratio, it is the same as ``(w / h) * sar``\n",
            "             - hsub: horizontal chroma subsample values. For example for the pixel format \"yuv422p\" hsub is 2 and vsub\n",
            "               is 1.\n",
            "             - vsub: vertical chroma subsample values. For example for the pixel format \"yuv422p\" hsub is 2 and vsub\n",
            "               is 1.\n",
            "             - line_h: the height of each text line\n",
            "             - lh: Alias for ``line_h``.\n",
            "             - main_h: the input height\n",
            "             - h: Alias for ``main_h``.\n",
            "             - H: Alias for ``main_h``.\n",
            "             - main_w: the input width\n",
            "             - w: Alias for ``main_w``.\n",
            "             - W: Alias for ``main_w``.\n",
            "             - ascent: the maximum distance from the baseline to the highest/upper grid coordinate used to place a glyph\n",
            "               outline point, for all the rendered glyphs. It is a positive value, due to the grid's orientation with the Y\n",
            "               axis upwards.\n",
            "             - max_glyph_a: Alias for ``ascent``.\n",
            "             - descent: the maximum distance from the baseline to the lowest grid coordinate used to place a glyph outline\n",
            "               point, for all the rendered glyphs. This is a negative value, due to the grid's orientation, with the Y axis\n",
            "               upwards.\n",
            "             - max_glyph_d: Alias for ``descent``.\n",
            "             - max_glyph_h: maximum glyph height, that is the maximum height for all the glyphs contained in the rendered\n",
            "               text, it is equivalent to ascent - descent.\n",
            "             - max_glyph_w: maximum glyph width, that is the maximum width for all the glyphs contained in the rendered\n",
            "               text.\n",
            "             - n: the number of input frame, starting from 0\n",
            "             - rand(min, max): return a random number included between min and max\n",
            "             - sar: The input sample aspect ratio.\n",
            "             - t: timestamp expressed in seconds, NAN if the input timestamp is unknown\n",
            "             - text_h: the height of the rendered text\n",
            "             - th: Alias for ``text_h``.\n",
            "             - text_w: the width of the rendered text\n",
            "             - tw: Alias for ``text_w``.\n",
            "             - x: the x offset coordinates where the text is drawn.\n",
            "             - y: the y offset coordinates where the text is drawn.\n",
            "        \n",
            "            These parameters allow the x and y expressions to refer each other, so you can for example specify\n",
            "            ``y=x/dar``.\n",
            "        \n",
            "        Official documentation: `drawtext <https://ffmpeg.org/ffmpeg-filters.html#drawtext>`__\n",
            "    \n",
            "    filter(stream_spec, filter_name, *args, **kwargs)\n",
            "        Apply custom filter.\n",
            "        \n",
            "        ``filter_`` is normally used by higher-level filter functions such as ``hflip``, but if a filter implementation\n",
            "        is missing from ``fmpeg-python``, you can call ``filter_`` directly to have ``fmpeg-python`` pass the filter name\n",
            "        and arguments to ffmpeg verbatim.\n",
            "        \n",
            "        Args:\n",
            "            stream_spec: a Stream, list of Streams, or label-to-Stream dictionary mapping\n",
            "            filter_name: ffmpeg filter name, e.g. `colorchannelmixer`\n",
            "            *args: list of args to pass to ffmpeg verbatim\n",
            "            **kwargs: list of keyword-args to pass to ffmpeg verbatim\n",
            "        \n",
            "        The function name is suffixed with ``_`` in order avoid confusion with the standard python ``filter`` function.\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "            ``ffmpeg.input('in.mp4').filter('hflip').output('out.mp4').run()``\n",
            "    \n",
            "    filter_(stream_spec, filter_name, *args, **kwargs)\n",
            "        Alternate name for ``filter``, so as to not collide with the\n",
            "        built-in python ``filter`` operator.\n",
            "    \n",
            "    filter_multi_output(stream_spec, filter_name, *args, **kwargs)\n",
            "        Apply custom filter with one or more outputs.\n",
            "        \n",
            "        This is the same as ``filter_`` except that the filter can produce more than one output.\n",
            "        \n",
            "        To reference an output stream, use either the ``.stream`` operator or bracket shorthand:\n",
            "        \n",
            "        Example:\n",
            "        \n",
            "            ```\n",
            "            split = ffmpeg.input('in.mp4').filter_multi_output('split')\n",
            "            split0 = split.stream(0)\n",
            "            split1 = split[1]\n",
            "            ffmpeg.concat(split0, split1).output('out.mp4').run()\n",
            "            ```\n",
            "    \n",
            "    get_args(stream_spec, overwrite_output=False)\n",
            "        Build command-line arguments to be passed to ffmpeg.\n",
            "    \n",
            "    hflip(stream)\n",
            "        Flip the input video horizontally.\n",
            "        \n",
            "        Official documentation: `hflip <https://ffmpeg.org/ffmpeg-filters.html#hflip>`__\n",
            "    \n",
            "    hue(stream, **kwargs)\n",
            "        Modify the hue and/or the saturation of the input.\n",
            "        \n",
            "        Args:\n",
            "            h: Specify the hue angle as a number of degrees. It accepts an expression, and defaults to \"0\".\n",
            "            s: Specify the saturation in the [-10,10] range. It accepts an expression and defaults to \"1\".\n",
            "            H: Specify the hue angle as a number of radians. It accepts an expression, and defaults to \"0\".\n",
            "            b: Specify the brightness in the [-10,10] range. It accepts an expression and defaults to \"0\".\n",
            "        \n",
            "        Official documentation: `hue <https://ffmpeg.org/ffmpeg-filters.html#hue>`__\n",
            "    \n",
            "    input(filename, **kwargs)\n",
            "        Input file URL (ffmpeg ``-i`` option)\n",
            "        \n",
            "        Any supplied kwargs are passed to ffmpeg verbatim (e.g. ``t=20``,\n",
            "        ``f='mp4'``, ``acodec='pcm'``, etc.).\n",
            "        \n",
            "        To tell ffmpeg to read from stdin, use ``pipe:`` as the filename.\n",
            "        \n",
            "        Official documentation: `Main options <https://ffmpeg.org/ffmpeg.html#Main-options>`__\n",
            "    \n",
            "    merge_outputs(*streams)\n",
            "        Include all given outputs in one ffmpeg command line\n",
            "    \n",
            "    output(*streams_and_filename, **kwargs)\n",
            "        Output file URL\n",
            "        \n",
            "        Syntax:\n",
            "            `ffmpeg.output(stream1[, stream2, stream3...], filename, **ffmpeg_args)`\n",
            "        \n",
            "        Any supplied keyword arguments are passed to ffmpeg verbatim (e.g.\n",
            "        ``t=20``, ``f='mp4'``, ``acodec='pcm'``, ``vcodec='rawvideo'``,\n",
            "        etc.).  Some keyword-arguments are handled specially, as shown below.\n",
            "        \n",
            "        Args:\n",
            "            video_bitrate: parameter for ``-b:v``, e.g. ``video_bitrate=1000``.\n",
            "            audio_bitrate: parameter for ``-b:a``, e.g. ``audio_bitrate=200``.\n",
            "            format: alias for ``-f`` parameter, e.g. ``format='mp4'``\n",
            "                (equivalent to ``f='mp4'``).\n",
            "        \n",
            "        If multiple streams are provided, they are mapped to the same\n",
            "        output.\n",
            "        \n",
            "        To tell ffmpeg to write to stdout, use ``pipe:`` as the filename.\n",
            "        \n",
            "        Official documentation: `Synopsis <https://ffmpeg.org/ffmpeg.html#Synopsis>`__\n",
            "    \n",
            "    overlay(main_parent_node, overlay_parent_node, eof_action='repeat', **kwargs)\n",
            "        Overlay one video on top of another.\n",
            "        \n",
            "        Args:\n",
            "            x: Set the expression for the x coordinates of the overlaid video on the main video. Default value is 0. In\n",
            "                case the expression is invalid, it is set to a huge value (meaning that the overlay will not be displayed\n",
            "                within the output visible area).\n",
            "            y: Set the expression for the y coordinates of the overlaid video on the main video. Default value is 0. In\n",
            "                case the expression is invalid, it is set to a huge value (meaning that the overlay will not be displayed\n",
            "                within the output visible area).\n",
            "            eof_action: The action to take when EOF is encountered on the secondary input; it accepts one of the following\n",
            "                values:\n",
            "        \n",
            "                * ``repeat``: Repeat the last frame (the default).\n",
            "                * ``endall``: End both streams.\n",
            "                * ``pass``: Pass the main input through.\n",
            "        \n",
            "            eval: Set when the expressions for x, and y are evaluated.\n",
            "                It accepts the following values:\n",
            "        \n",
            "                * ``init``: only evaluate expressions once during the filter initialization or when a command is\n",
            "                    processed\n",
            "                * ``frame``: evaluate expressions for each incoming frame\n",
            "        \n",
            "                Default value is ``frame``.\n",
            "            shortest: If set to 1, force the output to terminate when the shortest input terminates. Default value is 0.\n",
            "            format: Set the format for the output video.\n",
            "                It accepts the following values:\n",
            "        \n",
            "                * ``yuv420``: force YUV420 output\n",
            "                * ``yuv422``: force YUV422 output\n",
            "                * ``yuv444``: force YUV444 output\n",
            "                * ``rgb``: force packed RGB output\n",
            "                * ``gbrp``: force planar RGB output\n",
            "        \n",
            "                Default value is ``yuv420``.\n",
            "            rgb (deprecated): If set to 1, force the filter to accept inputs in the RGB color space. Default value is 0.\n",
            "                This option is deprecated, use format instead.\n",
            "            repeatlast: If set to 1, force the filter to draw the last overlay frame over the main input until the end of\n",
            "                the stream. A value of 0 disables this behavior. Default value is 1.\n",
            "        \n",
            "        Official documentation: `overlay <https://ffmpeg.org/ffmpeg-filters.html#overlay-1>`__\n",
            "    \n",
            "    overwrite_output(stream)\n",
            "        Overwrite output files without asking (ffmpeg ``-y`` option)\n",
            "        \n",
            "        Official documentation: `Main options <https://ffmpeg.org/ffmpeg.html#Main-options>`__\n",
            "    \n",
            "    probe(filename, cmd='ffprobe')\n",
            "        Run ffprobe on the specified file and return a JSON representation of the output.\n",
            "        \n",
            "        Raises:\n",
            "            :class:`ffmpeg.Error`: if ffprobe returns a non-zero exit code,\n",
            "                an :class:`Error` is returned with a generic error message.\n",
            "                The stderr output can be retrieved by accessing the\n",
            "                ``stderr`` property of the exception.\n",
            "    \n",
            "    run(stream_spec, cmd='ffmpeg', capture_stdout=False, capture_stderr=False, input=None, quiet=False, overwrite_output=False)\n",
            "        Invoke ffmpeg for the supplied node graph.\n",
            "        \n",
            "        Args:\n",
            "            capture_stdout: if True, capture stdout (to be used with\n",
            "                ``pipe:`` ffmpeg outputs).\n",
            "            capture_stderr: if True, capture stderr.\n",
            "            quiet: shorthand for setting ``capture_stdout`` and ``capture_stderr``.\n",
            "            input: text to be sent to stdin (to be used with ``pipe:``\n",
            "                ffmpeg inputs)\n",
            "            **kwargs: keyword-arguments passed to ``get_args()`` (e.g.\n",
            "                ``overwrite_output=True``).\n",
            "        \n",
            "        Returns: (out, err) tuple containing captured stdout and stderr data.\n",
            "    \n",
            "    run_async(stream_spec, cmd='ffmpeg', pipe_stdin=False, pipe_stdout=False, pipe_stderr=False, quiet=False, overwrite_output=False)\n",
            "        Asynchronously invoke ffmpeg for the supplied node graph.\n",
            "        \n",
            "        Args:\n",
            "            pipe_stdin: if True, connect pipe to subprocess stdin (to be\n",
            "                used with ``pipe:`` ffmpeg inputs).\n",
            "            pipe_stdout: if True, connect pipe to subprocess stdout (to be\n",
            "                used with ``pipe:`` ffmpeg outputs).\n",
            "            pipe_stderr: if True, connect pipe to subprocess stderr.\n",
            "            quiet: shorthand for setting ``capture_stdout`` and\n",
            "                ``capture_stderr``.\n",
            "            **kwargs: keyword-arguments passed to ``get_args()`` (e.g.\n",
            "                ``overwrite_output=True``).\n",
            "        \n",
            "        Returns:\n",
            "            A `subprocess Popen`_ object representing the child process.\n",
            "        \n",
            "        Examples:\n",
            "            Run and stream input::\n",
            "        \n",
            "                process = (\n",
            "                    ffmpeg\n",
            "                    .input('pipe:', format='rawvideo', pix_fmt='rgb24', s='{}x{}'.format(width, height))\n",
            "                    .output(out_filename, pix_fmt='yuv420p')\n",
            "                    .overwrite_output()\n",
            "                    .run_async(pipe_stdin=True)\n",
            "                )\n",
            "                process.communicate(input=input_data)\n",
            "        \n",
            "            Run and capture output::\n",
            "        \n",
            "                process = (\n",
            "                    ffmpeg\n",
            "                    .input(in_filename)\n",
            "                    .output('pipe':, format='rawvideo', pix_fmt='rgb24')\n",
            "                    .run_async(pipe_stdout=True, pipe_stderr=True)\n",
            "                )\n",
            "                out, err = process.communicate()\n",
            "        \n",
            "            Process video frame-by-frame using numpy::\n",
            "        \n",
            "                process1 = (\n",
            "                    ffmpeg\n",
            "                    .input(in_filename)\n",
            "                    .output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
            "                    .run_async(pipe_stdout=True)\n",
            "                )\n",
            "        \n",
            "                process2 = (\n",
            "                    ffmpeg\n",
            "                    .input('pipe:', format='rawvideo', pix_fmt='rgb24', s='{}x{}'.format(width, height))\n",
            "                    .output(out_filename, pix_fmt='yuv420p')\n",
            "                    .overwrite_output()\n",
            "                    .run_async(pipe_stdin=True)\n",
            "                )\n",
            "        \n",
            "                while True:\n",
            "                    in_bytes = process1.stdout.read(width * height * 3)\n",
            "                    if not in_bytes:\n",
            "                        break\n",
            "                    in_frame = (\n",
            "                        np\n",
            "                        .frombuffer(in_bytes, np.uint8)\n",
            "                        .reshape([height, width, 3])\n",
            "                    )\n",
            "                    out_frame = in_frame * 0.3\n",
            "                    process2.stdin.write(\n",
            "                        frame\n",
            "                        .astype(np.uint8)\n",
            "                        .tobytes()\n",
            "                    )\n",
            "        \n",
            "                process2.stdin.close()\n",
            "                process1.wait()\n",
            "                process2.wait()\n",
            "        \n",
            "        .. _subprocess Popen: https://docs.python.org/3/library/subprocess.html#popen-objects\n",
            "    \n",
            "    setpts(stream, expr)\n",
            "        Change the PTS (presentation timestamp) of the input frames.\n",
            "        \n",
            "        Args:\n",
            "            expr: The expression which is evaluated for each frame to construct its timestamp.\n",
            "        \n",
            "        Official documentation: `setpts, asetpts <https://ffmpeg.org/ffmpeg-filters.html#setpts_002c-asetpts>`__\n",
            "    \n",
            "    trim(stream, **kwargs)\n",
            "        Trim the input so that the output contains one continuous subpart of the input.\n",
            "        \n",
            "        Args:\n",
            "            start: Specify the time of the start of the kept section, i.e. the frame with the timestamp start will be the\n",
            "                first frame in the output.\n",
            "            end: Specify the time of the first frame that will be dropped, i.e. the frame immediately preceding the one\n",
            "                with the timestamp end will be the last frame in the output.\n",
            "            start_pts: This is the same as start, except this option sets the start timestamp in timebase units instead of\n",
            "                seconds.\n",
            "            end_pts: This is the same as end, except this option sets the end timestamp in timebase units instead of\n",
            "                seconds.\n",
            "            duration: The maximum duration of the output in seconds.\n",
            "            start_frame: The number of the first frame that should be passed to the output.\n",
            "            end_frame: The number of the first frame that should be dropped.\n",
            "        \n",
            "        Official documentation: `trim <https://ffmpeg.org/ffmpeg-filters.html#trim>`__\n",
            "    \n",
            "    vflip(stream)\n",
            "        Flip the input video vertically.\n",
            "        \n",
            "        Official documentation: `vflip <https://ffmpeg.org/ffmpeg-filters.html#vflip>`__\n",
            "    \n",
            "    view(stream_spec, detail=False, filename=None, pipe=False, **kwargs)\n",
            "    \n",
            "    zoompan(stream, **kwargs)\n",
            "        Apply Zoom & Pan effect.\n",
            "        \n",
            "        Args:\n",
            "            zoom: Set the zoom expression. Default is 1.\n",
            "            x: Set the x expression. Default is 0.\n",
            "            y: Set the y expression. Default is 0.\n",
            "            d: Set the duration expression in number of frames. This sets for how many number of frames effect will last\n",
            "                for single input image.\n",
            "            s: Set the output image size, default is ``hd720``.\n",
            "            fps: Set the output frame rate, default is 25.\n",
            "            z: Alias for ``zoom``.\n",
            "        \n",
            "        Official documentation: `zoompan <https://ffmpeg.org/ffmpeg-filters.html#zoompan>`__\n",
            "\n",
            "DATA\n",
            "    __all__ = ['colorchannelmixer', 'concat', 'crop', 'drawbox', 'drawtext...\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.6/dist-packages/ffmpeg/__init__.py\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a5Zo2OAYWw3R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "cd73a39f-acf1-435e-9d7a-2aca08c8feaa"
      },
      "cell_type": "code",
      "source": [
        "stream = ffmpeg.input('detected_video.mp4')\n",
        "#stream = ffmpeg.filter(stream, 'fps', fps=25, round='up')\n",
        "stream = ffmpeg.output(stream, 'detected_compressed.mp4',**{'vcodec': libx264,'crf': 24})\n",
        "ffmpeg.run(stream)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b984b13a328d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'detected_video.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#stream = ffmpeg.filter(stream, 'fps', fps=25, round='up')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'detected_compressed.mp4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'vcodec'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlibx264\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'crf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mffmpeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'ffmpeg' has no attribute 'input'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7ccQakrtVJN0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1bf08d9f-4648-4667-d5ab-80cfe70edfab"
      },
      "cell_type": "code",
      "source": [
        "ffmpeg.input('detected_video.mp4').output('detected_compressed.mp4',**{'vcodec': \"libx264\",'crf': 24}).run()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "gItLWxGLZh9y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e379ed99-f198-4511-89da-f84629b20e14"
      },
      "cell_type": "code",
      "source": [
        "ffmpeg.input('top_10.mp4').output('detected-audio.aac',**{'acodec': \"copy\"}).run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "metadata": {
        "id": "RYhwdk7_w-LT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ebf2a3f-a1fb-4ca6-ea2a-b9315ba0387e"
      },
      "cell_type": "code",
      "source": [
        "stream1=ffmpeg.input('detected_compressed.mp4')\n",
        "stream2=ffmpeg.input('detected-audio.aac')\n",
        "#stream=ffmpeg.concat(stream1,stream2)\n",
        "stream=ffmpeg.output(stream1,stream2,'detected_compressed_with_audio.mp4',**{'c:v': \"copy\",'c:a': \"aac\",'b:a': \"256k\"})\n",
        "ffmpeg.run(stream)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "metadata": {
        "id": "BtsiaFjW3gI-"
      },
      "cell_type": "code",
      "source": [
        "'''test_file=cv2.imread('test.png')\n",
        "\n",
        "test_det=detectlogo(logo_model,test_file,p)\n",
        "cv2.imshow('ball_player',test_det)\n",
        "\n",
        "key = cv2.waitKey(0)'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I2cBCkGm3gJB",
        "outputId": "0dd9865c-5e37-49c7-ffc0-117f96ea731a"
      },
      "cell_type": "code",
      "source": [
        "'''cap = cv2.VideoCapture('top_10.mp4')\n",
        "frameId = cap.get(1)\n",
        "count = 0\n",
        "while(True):\n",
        "  #tic=time.time()\n",
        "  #time.sleep(1)\n",
        "  cap.set(1,frameId+30)\n",
        "  ret, frame = cap.read(1)\n",
        "  frameId = cap.get(1)\n",
        "  test_det=detectlogo(logo_model,frame,p)\n",
        "  cv2.imshow('ball_player_logo',test_det)\n",
        "  #cv2.imshow(\"fifa\",frame)\n",
        "  key = cv2.waitKey(1) & 0xFF\n",
        "  if key == ord(\"q\"):\n",
        "    break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()'''\n",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ELQjsEQD3gJE"
      },
      "cell_type": "code",
      "source": [
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9n_0b2Mj3gJI"
      },
      "cell_type": "code",
      "source": [
        "'''p = dict()\n",
        "p['l2n'] = {0: 'adidas', 1: 'aldi', 2: 'apple', 3: 'becks', 4: 'bmw', 5: 'carlsberg', 6: 'chimay', 7: 'cocacola', 8: 'corona', 9: 'dhl', 10: 'erdinger', 11: 'esso', 12: 'fedex', 13: 'ferrari', 14: 'ford', 15: 'fosters', 16: 'google', 17: 'guiness', 18: 'heineken', 19: 'HP', 20: 'milka', 21: 'nvidia', 22: 'paulaner', 23: 'pepsi', 24: 'rittersport', 25: 'shell', 26: 'singha', 27: 'starbucks', 28: 'stellaartois', 29: 'texaco', 30: 'tsingtao', 31: 'ups'}\n",
        "#model, training_model, prediction_model = create_models_local(p)\n",
        "\n",
        "showlogo(logo_model, 'test.png','out.test.png',p)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OE2CmpiL3gJM"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nybb8Fzi3gJT"
      },
      "cell_type": "code",
      "source": [
        "#prediction_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ba-6igp3gJX"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}